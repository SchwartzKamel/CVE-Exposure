from pandas.core import base
import requests
from bs4 import BeautifulSoup
import pandas as pd


class FeedScraper():
    def __init__(self) -> None:
        pass

    ### WEB SCRAPER FUNCTIONALITY ###

    def get_soup(self, base_url):
        r = requests.get(base_url)
        soup = BeautifulSoup(r.text, "html.parser")

        return soup

    # TODO replace with threading or asyncio for better speeds
    def requester(self, article_links, div_type, query_item):
        article_text = []
        for i in range(len(article_links)):
            article = self.get_soup(article_links[i])

            if div_type == "class":
                text = self.div_class_article_text_lookup(
                    article, query_item=query_item)
                article_text.append(text)
            elif div_type == "id":
                text = self.div_id_article_text_lookup(
                    article, query_item=query_item)
                article_text.append(text)
            else:
                break
        # convert scraped text to pandas dataframe
        df_article_text = pd.DataFrame(article_text)
        # transpose df, and turned into a single column
        df_article_text = df_article_text.transpose()
        df_article_text = df_article_text.stack().reset_index()
        df_article_text.columns = ['0', '1', 'text']
        df_article_text = df_article_text[['text']]

        return df_article_text

    ### PARSERS ###

    def div_id_link_lookup(self, soup, query_item):
        site_info = soup.find('div', id=query_item)
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in site_info.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe links(TODO)
        df = pd.DataFrame(raw_links)
        df.columns = ['Links']
        # return as a list
        article_links = df['Links'].to_list()

        return article_links

    def div_id_article_text_lookup(self, soup, query_item):
        site_info = soup.find('div', id=query_item)
        # find text in articles append to list as text
        article_text = []
        for text in site_info.find_all('p'):
            article_text.append(str(text))

        return article_text

    def div_class_link_lookup(self, soup, query_item):
        site_info = soup.find('div', class_=query_item)
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in site_info.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe links(TODO)
        df = pd.DataFrame(raw_links)
        df.columns = ['Links']
        # return as a list
        article_links = df['Links'].to_list()

        return article_links

    def div_class_article_text_lookup(self, soup, query_item):
        site_info = soup.find('div', class_=query_item)
        # find text in articles append to list as text
        article_text = []
        for text in site_info.find_all('p'):
            article_text.append(str(text))

        return article_text

    def cve_parser(self, df_article_text, site):
        # filter down data and extract CVE numbers
        df_cve = df_article_text.loc[df_article_text['text'].str.contains(
            'CVE-', case=True)]
        df_cve = df_cve['text'].str.extract(r'(CVE-[\d]{4}-[\d]{1,})')
        df_cve.columns = ['CVE']
        df_cve['Site'] = site
        df_cve = df_cve.pivot_table(index=['Site', 'CVE'], aggfunc='size')

        return df_cve

    ### NEWS SITES ###

    def bleeping_computer(self):
        """
        1. Need to remove author links and #comment-form links
        2. parse for CVE
        """
        bc_home = self.get_soup(
            base_url="https://www.bleepingcomputer.com/news/security/")
        bc_links = self.div_class_link_lookup(bc_home, "bc_latest_news")
        df_bc_text = self.requester(bc_links, "class", "article_section")

        return df_bc_text

    def hacker_news(self):
        hn_home = self.get_soup(base_url="https://thehackernews.com/")
        hn_links = self.div_class_link_lookup(hn_home, "blog-posts clear")
        # df_hn_text = self.requester(hn_links, 'id', 'articlebody')
        # df_hn_cve = self.cve_parser(df_hn_text, "Hacker News")

        return hn_links

    def infosecurity_magazine(self):
        """
        1. Need to remove author links and #comment-form links
        2. iterate over links, find div class="article-body"
        """
        ism_home = self.get_soup(
            base_url="https://www.infosecurity-magazine.com/news/")
        ism_links = self.div_id_link_lookup(ism_home, "webpages-list")
        df_ism_text = self.requester(ism_links, "class", "article-body")

        return df_ism_text

    def threat_post(self):
        """
        1. Need to remove author links
        2. iterate over links, find div class="c-article_main"
        """
        tp_home = self.get_soup(
            base_url="https://threatpost.com/category/vulnerabilities/")
        tp_links = self.div_class_link_lookup(tp_home, "o-col")
        df_tp_text = self.requester(tp_links, "class", "c-article_main")

        return df_tp_text

    def zeroday_fans(self):
        """
        TODO
        """
        zdf_home = self.get_soup(base_url="https://0dayfans.com/")
        zdf_links = self.div_class_link_lookup(zdf_home, "post__title")

        return zdf_links

    def zeroday_initiative(self):
        zdi_home = self.get_soup(
            base_url="https://www.zerodayinitiative.com/blog")
        zdi_links = self.div_class_link_lookup(zdi_home, "section")
        df_zdi_text = self.requester(zdi_links, "class", "sqs-block-content")
        df_zdi_cve = self.cve_parser(df_zdi_text, "ZeroDay Initiative")

        return df_zdi_cve

    def it_pro(self):
        itp_home = self.get_soup(
            base_url="https://www.itpro.co.uk/zero-day-exploit")
        itp_links = self.div_class_link_lookup(
            itp_home, "polaris__article-card -layout-default -default polaris__article-group--single")
        df_itp_text = self.requester(
            itp_links, "class", "polaris__simple-grid--main")

        return df_itp_text

    def krebs(self):
        krebs_home = self.get_soup(base_url="https://krebsonsecurity.com/")
        krebs_links = self.div_id_link_lookup(krebs_home, "content")
        df_krebs_text = self.requester(krebs_links, "class", "entry")

        return df_krebs_text

    def reddit(self):
        """
        TODO
        """
        pass

    def darkreading_threat_intel(self):
        dr_ti_home = self.get_soup(
            base_url="https://www.darkreading.com/threat-intelligence.asp")
        dr_ti_links = self.div_class_link_lookup(dr_ti_home, "strong medium")
        df_dr_ti = self.requester(dr_ti_links, "id", "article-main")

        return df_dr_ti

    def darkreading_vuln_management(self):
        dr_vm_home = self.get_soup(
            base_url="https://www.darkreading.com/vulnerability-management.asp")
        dr_vm_links = self.div_class_link_lookup(dr_vm_home, "strong medium")
        df_dr_vm = self.requester(dr_vm_links, "id", "article-main")

        return df_dr_vm

    def we_live_security(self):
        wls_home = self.get_soup(base_url="https://www.welivesecurity.com/")
        wls_links = self.div_class_link_lookup(
            wls_home, "text-wrapper col-sm-9 col-xs-8 no-padding")
        df_wls_text = self.requester(
            wls_links, "class", "col-md-10 col-sm-10 col-xs-12 formatted")

        return df_wls_text

    def helpnetsecurity(self):
        """
        TODO
        """
        hns_home = self.get_soup(
            base_url="https://www.helpnetsecurity.com/view/news/")
        hns_links = self.div_class_link_lookup(hns_home, "entry-preview__body")

        return hns_links

    def trendmicro(self):
        tm_home = self.get_soup(
            base_url="https://www.trendmicro.com/vinfo/us/security/news/all/page/1")
        tm_links = self.div_class_link_lookup(
            tm_home, "responsiveColumnControl section")
        df_tm_text = self.requester(tm_links, "class", "articleContent")

        return df_tm_text

    def security_week(self):
        sw_home = self.get_soup(
            base_url="https://www.securityweek.com/virus-threats/vulnerabilities")
        sw_links = self.div_class_link_lookup(sw_home, "view-content")
        df_sw_text = self.requester(sw_links, "class", "content clear-block")

        return df_sw_text

    def cso(self):
        cso_home = self.get_soup(
            base_url="https://www.csoonline.com/category/vulnerabilities/")
        cso_links = self.div_class_link_lookup(cso_home, "main-col")
        df_cso_text = self.requester(cso_links, "id", "drr-container")

        return df_cso_text
