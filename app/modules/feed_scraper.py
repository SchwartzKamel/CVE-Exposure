import requests
import json
from bs4 import BeautifulSoup
import raccoon as rc


class FeedScraper():
    def __init__(self) -> None:
        pass

    ### WEB SCRAPER FUNCTIONALITY ###

    def get_soup(self, base_url):
        r = requests.get(base_url)
        soup = BeautifulSoup(r.text, "html.parser")

        return soup

    def requester(self, article_links, div_type, query_item):
        article = []
        for i in range(len(article_links)):
            article = self.get_soup(article_links[i])

            if div_type == "class":
                article_text = self.div_class_article_text_lookup(
                    article, query_item=query_item)
            elif div_type == "id":
                article_text = self.div_id_article_text_lookup(
                    article, query_item=query_item)
            else:
                break

        return article_text

    ### PARSERS ###

    def div_id_link_lookup(self, soup, query_item):
        site_info = soup.find('div', id=query_item)
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in site_info.find_all('a'):
            if link():
                raw_links.append(link.get('href'))
        # de-dupe links and return as a list
        article_links = list(set(raw_links))

        return article_links

    def div_id_article_text_lookup(self, soup, query_item):
        site_info = soup.find('div', id=query_item)
        # find links to articles in news listing, append to list of links
        article_text = []
        for link in site_info.find_all('p'):
            if link():
                article_text.append(link)

        return article_text

    def div_class_link_lookup(self, soup, query_item):
        site_info = soup.find('div', class_=query_item)
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in site_info.find_all('a'):
            if link():
                raw_links.append(link.get('href'))
        # de-dupe links and return as a list
        article_links = list(set(raw_links))

        return article_links

    def div_class_article_text_lookup(self, soup, query_item):
        site_info = soup.find('div', class_=query_item)
        # find links to articles in news listing, append to list of links
        article_text = []
        for link in site_info.find_all('p'):
            if link():
                article_text.append(link)

        return article_text

    ### NEWS SITES ###

    def bleeping_computer(self):
        """
        1. Need to remove author links and #comment-form links
        2. parse for CVE
        """
        bc_home = self.get_soup(
            base_url="https://www.bleepingcomputer.com/news/security/")
        bc_links = self.div_class_link_lookup(bc_home, "bc_latest_news")
        # TODO replace with for loop to be lazy or multi-thread to be cool
        bc_article_text = self.requester(bc_links, "class", "article_section")
        # article text needs to include all article's text
        # convert to raccoon dataframe TODO
        df_bc_text = bc_article_text

        return df_bc_text

    def hacker_news(self):
        """
        1. iterate over links, find div id="articlebody"
        2. parse for CVE
        """
        hn_home = self.get_soup(base_url="https://thehackernews.com/")
        hn_links = self.div_class_link_lookup(hn_home, "blog-posts clear")
        hn_article_text = self.requester(hn_links, "id", "articlebody")
        # article text needs to include all article's text
        # convert to raccoon dataframe TODO
        df_hn_text = hn_article_text

        return df_hn_text

    def infosecurity_magazine(self):
        """
        1. Need to remove author links and #comment-form links
        2. iterate over links, find div class="article-body"
        """
        ism_home = self.get_soup(
            base_url="https://www.infosecurity-magazine.com/news/")
        ism_links = self.div_id_link_lookup(ism_home, "webpages-list")
        ism_article_text = self.requester(ism_links, "class", "article-body")

        return ism_article_text

    def threat_post(self):
        """
        1. Need to remove author links
        2. iterate over links, find div class="c-article_main"
        """
        tp_home = self.get_soup(
            base_url="https://threatpost.com/category/vulnerabilities/")
        tp_links = self.div_class_link_lookup(tp_home, "o-col")
        tp_article_text = self.requester(tp_links, "class", "c-article_main")

        return tp_article_text

    def zeroday_fans(self):
        pass

    def zeroday_initiative(self):
        pass

    def it_pro(self):
        pass

    def krebs(self):
        pass

    def reddit(self):
        pass

    def darkreading(self):
        pass

    def we_live_security(self):
        pass

    def helpnetsecurity(self):
        pass

    def trendmicro(self):
        pass

    def security_week(self):
        pass

    def cso(self):
        pass
