import requests
from bs4 import BeautifulSoup
import pandas as pd


class FeedScraper():
    def __init__(self) -> None:
        pass

    ### WEB SCRAPER FUNCTIONALITY ###

    def get_soup(base_url):
        r = requests.get(base_url)
        soup = BeautifulSoup(r.text, "html.parser")

        return soup

    # TODO replace with threading or asyncio for better speeds
    def requester(self, article_links, div_type, query_item):
        article_text = []
        for i in range(len(article_links)):
            article = self.get_soup(article_links[i])

            if div_type == "class":
                text = self.div_class_article_text_lookup(
                    article, query_item=query_item)
                article_text.append(text)
            elif div_type == "id":
                text = self.div_id_article_text_lookup(
                    article, query_item=query_item)
                article_text.append(text)
            else:
                break
        # convert scraped text to pandas dataframe
        df_article_text = pd.DataFrame(article_text)
        # transpose df, and turned into a single column
        df_article_text = df_article_text.transpose()
        df_article_text = df_article_text.stack().reset_index()
        df_article_text.columns = ['0', '1', 'text']
        df_article_text = df_article_text[['text']]

        return df_article_text

    ### PARSERS ###

    def div_id_link_lookup(soup, query_item):
        site_info = soup.find('div', id=query_item)
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in site_info.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe & clean links
        df = pd.DataFrame(raw_links)
        df.columns = ['Links']
        df.sort_values('Links', inplace=True)
        df.drop_duplicates(subset='Links', keep='first', inplace=True)
        df = df[~df['Links'].str.contains('author|comment|#modal|page/')]
        # return as a list
        article_links = df['Links'].to_list()

        return article_links

    def div_id_article_text_lookup(soup, query_item):
        site_info = soup.find('div', id=query_item)
        # find text in articles append to list as text
        article_text = []
        for text in site_info.find_all('p'):
            article_text.append(str(text))

        return article_text

    def div_class_link_lookup(soup, query_item):
        site_info = soup.find('div', class_=query_item)
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in site_info.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe & clean links
        df = pd.DataFrame(raw_links)
        df.columns = ['Links']
        df.sort_values('Links', inplace=True)
        df.drop_duplicates(subset='Links', keep='first', inplace=True)
        df = df[~df['Links'].str.contains('author|comment|#modal|page/')]
        # return as a list
        article_links = df['Links'].to_list()

        return article_links

    def div_class_article_text_lookup(soup, query_item):
        site_info = soup.find('div', class_=query_item)
        # find text in articles append to list as text
        article_text = []
        for text in site_info.find_all('p'):
            article_text.append(str(text))

        return article_text

    def cve_parser(df_article_text, site):
        # filter down data and extract CVE numbers
        df_cve = df_article_text.loc[df_article_text['text'].str.contains(
            'CVE-', case=True)]
        df_cve = df_cve['text'].str.extractall(r'(CVE-[\d]{4}-[\d]{1,})')
        df_cve.columns = ['CVE']
        df_cve['Site'] = site
        df_cve = df_cve.pivot_table(
            index=['Site', 'CVE'], aggfunc='size')

        return df_cve
