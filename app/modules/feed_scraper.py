import requests
from bs4 import BeautifulSoup


class FeedScraper():
    def __init__(self) -> None:
        pass

    def bleeping_computer(self):
        base_url = "https://www.bleepingcomputer.com/news/security/"
        # Read the first page and pass it as variable 'soup'
        base_r = requests.get(base_url)
        soup = BeautifulSoup(base_r.text, "html.parser")
        # find div class="bc_latest_news"
        bc_latest_news = soup.find('div', class_="bc_latest_news")
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in bc_latest_news.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe links and return as a list
        article_links = list(set(raw_links))

        return article_links

    def hacker_news(self):
        base_url = "https://thehackernews.com/"
        # Read the first page and pass it as variable 'soup'
        base_r = requests.get(base_url)
        soup = BeautifulSoup(base_r.text, "html.parser")
        # find div class="content section"
        hacker_news_content = soup.find('div', id="content")
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in hacker_news_content.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe links and return as a list
        article_links = list(set(raw_links))

        return article_links

    def infosecurity_magazine(self):
        base_url = "https://www.infosecurity-magazine.com/news/"
        # Read the first page and pass it as variable 'soup'
        base_r = requests.get(base_url)
        soup = BeautifulSoup(base_r.text, "html.parser")
        # find div id="webpages-list"
        infosec_magazine_news = soup.find('div', id="webpages-list")
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in infosec_magazine_news.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe links and return as a list
        article_links = list(set(raw_links))

        return article_links

    def threat_post(self):
        base_url = "https://threatpost.com/category/vulnerabilities/"
        # Read the first page and pass it as variable 'soup'
        base_r = requests.get(base_url)
        soup = BeautifulSoup(base_r.text, "html.parser")
        # find div class="o-col"
        threatpost_news = soup.find('div', class_="o-col")
        # find links to articles in news listing, append to list of links
        raw_links = []
        for link in threatpost_news.find_all('a'):
            raw_links.append(link.get('href'))
        # de-dupe links and return as a list
        article_links = list(set(raw_links))

        return article_links
