import requests
from bs4 import BeautifulSoup

base_url = "https://www.infosecurity-magazine.com/news/"

# Read the first page and pass it as variable 'soup'
base_r = requests.get(base_url)
soup = BeautifulSoup(base_r.text, "html.parser")

# find div id="webpages-list"
infosec_magazine_news = soup.find('div', id="webpages-list")

# find links to articles in news listing, append to list of links
raw_links = []
for link in infosec_magazine_news.find_all('a'):
    raw_links.append(link.get('href'))

article_links = list(set(raw_links))

print(*article_links, sep='\n')

# TODO
"""
1. Need to remove author links and #comment-form links
2. iterate over links, find div class="article-body"
3. parse data for CVE #
4. figure out way to track # of times a CVE appears per site (this needs to be in a function so we can return it to the main script)
"""
