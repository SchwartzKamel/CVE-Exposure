from modules.feed_scraper import FeedScraper as fs
import pandas as pd


class NewsSites(object):
    def __init__(self) -> None:
        pass
    ### NEWS SITES ###

    def bleeping_computer(self):
        bc_home = fs.get_soup(
            base_url="https://www.bleepingcomputer.com/news/security/")
        bc_links = fs.div_class_link_lookup(
            soup=bc_home, query_item="bc_latest_news")
        df_bc_text = fs.requester(fs,
                                  article_links=bc_links, div_type="class", query_item="article_section")
        df_bc_cve = fs.cve_parser(
            df_article_text=df_bc_text, site="Bleeping Computer")

        return df_bc_cve

    def hacker_news(self):
        hn_home = fs.get_soup(base_url="https://thehackernews.com/")
        hn_links = fs.div_class_link_lookup(
            soup=hn_home, query_item="blog-posts clear")
        df_hn_text = fs.requester(fs,
                                  article_links=hn_links, div_type='id', query_item='articlebody')
        df_hn_cve = fs.cve_parser(
            df_article_text=df_hn_text, site="Hacker News")

        return df_hn_cve

    def infosecurity_magazine(self):
        """
        TODO Need to interact w/ Javascript to work
        """
        ism_home = fs.get_soup(
            base_url="https://www.infosecurity-magazine.com/news/")
        ism_links = fs.div_id_link_lookup(
            soup=ism_home, query_item="webpages-list")
        df_ism_text = fs.requester(fs,
                                   article_links=ism_links, div_type="class", query_item="article-body")
        df_ism_cve = fs.cve_parser(
            df_article_text=df_ism_text, site="InfoSec Magazine")

        return df_ism_cve

    def threat_post(self):
        tp_home = fs.get_soup(
            base_url="https://threatpost.com/category/vulnerabilities/")
        tp_links = fs.div_class_link_lookup(soup=tp_home, query_item="o-col")
        df_tp_text = fs.requester(fs,
                                  article_links=tp_links, div_type="class", query_item="c-article__main")
        df_tp_cve = fs.cve_parser(
            df_article_text=df_tp_text, site="ThreatPost")

        return df_tp_cve

    def zeroday_fans(self):
        """
        TODO will need to way to ID which site is used from the article links
        """
        zdf_home = fs.get_soup(base_url="https://0dayfans.com/")
        zdf_links = fs.div_class_link_lookup(
            soup=zdf_home, query_item="post__title")

        return zdf_links

    def zeroday_initiative(self):
        """
        TODO links need to be appended to base url
        """
        zdi_home = fs.get_soup(
            base_url="https://www.zerodayinitiative.com/blog")
        zdi_links = fs.div_class_link_lookup(
            soup=zdi_home, query_item="section")
        # df_zdi_text = fs.requester(fs, zdi_links, div_type="class", query_item="sqs-block-content")
        # df_zdi_cve = fs.cve_parser(df_article_text=df_zdi_text, site="ZeroDay Initiative")

        return zdi_links

    def it_pro(self):
        """
        TODO links need to be appended to base url
        """
        itp_home = fs.get_soup(
            base_url="https://www.itpro.co.uk/zero-day-exploit")
        itp_links = fs.div_class_link_lookup(
            soup=itp_home, query_item="polaris__article-card -layout-default -default polaris__article-group--single")
        # df_itp_text = fs.requester(fs, article_links=itp_links, div_type="class", query_item="polaris__simple-grid--main")
        # df_itp_cve = fs.cve_parser(df_article_text=df_itp_text, site="IT Pro")

        return itp_links

    def krebs(self):
        """
        TODO parse for <p>, getting NONE_TYPE error
        """
        krebs_home = fs.get_soup(base_url="https://krebsonsecurity.com/")
        krebs_links = fs.div_id_link_lookup(
            soup=krebs_home, query_item="content")
        df_krebs_links = pd.DataFrame(krebs_links)
        df_krebs_links.columns = ['Links']
        df_krebs_links = df_krebs_links[df_krebs_links['Links'].str.contains(
            'krebsonsecurity')]
        krebs_links = df_krebs_links['Links'].to_list()
        test = fs.get_soup(base_url=krebs_links[0])
        # df_krebs_text = fs.requester(fs, article_links=krebs_links, div_type="class", query_item="entry")
        # df_krebs_cve = fs.cve_parser(df_article_text=df_krebs_text, site="KrebsOnSecurity")

        return test

    def reddit(self):
        """
        TODO
        """
        pass

    def darkreading_threat_intel(self):
        """
        TODO issue finding article links, unable to parse <a> elements
        """
        dr_ti_home = fs.get_soup(
            base_url="https://www.darkreading.com/threat-intelligence.asp")
        # dr_ti_links = fs.div_class_link_lookup(soup=dr_ti_home, query_item="strong medium")
        # df_dr_ti = fs.requester(fs, article_links=dr_ti_links, div_type="id", query_item="article-main")

        return dr_ti_home

    def darkreading_vuln_management(self):
        """
        TODO issue finding article links, unable to parse <a> elements
        """
        dr_vm_home = fs.get_soup(
            base_url="https://www.darkreading.com/vulnerability-management.asp")
        # dr_vm_links = fs.div_class_link_lookup(soup=dr_vm_home, query_item="strong medium")
        # df_dr_vm = fs.requester(article_links=dr_vm_links, div_type="id", query_item="article-main")

        return dr_vm_home

    def we_live_security(self):
        """
        TODO Need to figure out why it's not getting data
        """
        wls_home = fs.get_soup(
            base_url="https://www.welivesecurity.com/")
        wls_links = fs.div_class_link_lookup(
            soup=wls_home, query_item="text-wrapper col-sm-9 col-xs-8 no-padding")
        df_wls_text = fs.requester(fs,
                                   article_links=wls_links, div_type="class", query_item="col-md-10 col-sm-10 col-xs-12 formatted")
        df_wls_cve = fs.cve_parser(
            df_article_text=df_wls_text, site="We Live Security")

        return df_wls_cve

    def helpnetsecurity(self):
        """
        TODO Need to interact w/ Javascript
        """
        hns_home = fs.get_soup(
            base_url="https://www.helpnetsecurity.com/view/news/")
        hns_links = fs.div_class_link_lookup(
            soup=hns_home, query_item="entry-preview__body")

        return hns_links

    def trendmicro(self):
        """
        TODO links need to be appended to base url
        """
        tm_home = fs.get_soup(
            base_url="https://www.trendmicro.com/vinfo/us/security/news/all/page/1")
        tm_links = fs.div_class_link_lookup(
            soup=tm_home, query_item="prod-content")
        # df_tm_text = fs.requester(fs, article_links=tm_links, div_type="class", query_item="articleContent")

        return tm_links

    def security_week(self):
        """
        TODO links need to be appended to base url
        """
        sw_home = fs.get_soup(
            base_url="https://www.securityweek.com/virus-threats/vulnerabilities")
        sw_links = fs.div_id_link_lookup(soup=sw_home, query_item="center")
        # df_sw_text = fs.requester(fs, article_links=sw_links, div_type="class", query_item="content clear-block")

        return sw_links

    def cso(self):
        """
        TODO links need to be appended to base url
        """
        cso_home = fs.get_soup(
            base_url="https://www.csoonline.com/category/vulnerabilities/")
        cso_links = fs.div_class_link_lookup(
            soup=cso_home, query_item="main-col")
        # df_cso_text = fs.requester(fs, article_links=cso_links, div_type="id", query_item="drr-container")

        return cso_links

    def microsoft_security(self):
        """
        TODO issue parsing for <a> elements
        """
        ms_home = fs.get_soup(
            base_url="https://www.microsoft.com/security/blog/")
        ms_links = fs.div_id_link_lookup(
            soup=ms_home, query_item="posts-river-container")

        return ms_links
