import requests
from bs4 import BeautifulSoup

base_url = "https://www.bleepingcomputer.com/news/security/"

# Read the first page and pass it as variable 'soup'
base_r = requests.get(base_url)
soup = BeautifulSoup(base_r.text, "html.parser")

# find div class="bc_latest_news"
bc_latest_news = soup.find('div', class_="bc_latest_news")

# find links to articles in news listing, append to list of links
article_links = []
for link in bc_latest_news.find_all('a'):
    article_links.append(link.get('href'))

print(*article_links, sep='\n')

# TODO
"""
1. Need to de-dupe article links and remove author links
2. iterate over links, find div class="articleBody"
3. parse data for CVE #
4. figure out way to track # of times a CVE appears per site (this needs to be in a function so we can return it to the main script)
"""
