import requests
from bs4 import BeautifulSoup

base_url = "https://thehackernews.com/"

# Read the first page and pass it as variable 'soup'
base_r = requests.get(base_url)
soup = BeautifulSoup(base_r.text, "html.parser")

# find div class="content section"
hacker_news_content = soup.find('div', id="content")

# find links to articles in news listing, append to list of links
raw_links = []
for link in hacker_news_content.find_all('a'):
    raw_links.append(link.get('href'))

article_links = list(set(raw_links))

print(*article_links, sep='\n')

# TODO
"""
1. iterate over links, find div id="articlebody"
2. parse data for CVE #
3. figure out way to track # of times a CVE appears per site (this needs to be in a function so we can return it to the main script)
"""
